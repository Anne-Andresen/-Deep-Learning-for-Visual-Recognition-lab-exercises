{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOWjMqHz2nsx4DkBxf1w9w1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":36,"metadata":{"id":"tZ9RjDLfRYQI","executionInfo":{"status":"ok","timestamp":1695808451707,"user_tz":-120,"elapsed":492,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Lab 5 (Part 3): PyTorch modules and classes\n","Remember to set Runtime environment to GPU\n","\n","This tutorial is inspired by a tutorial made by Jeremy Howard from fast.ai. You can find the original tutorial here. And many other tutorials here.\n","\n","What is PyTorch?\n","PyTorch provides the elegantly designed modules and classes torch.nn, torch.optim, Dataset, and DataLoader to help you create and train neural networks.\n","\n","In order to fully utilize their power and customize them for your problem, you need to really understand exactly what they're doing. To develop this understanding, we will first train a basic neural net on the MNIST data set without using any features from these models; we will initially only use the most basic PyTorch tensor functionality. Then, we will incrementally add one feature from torch.nn, torch.optim, Dataset, or DataLoader at a time, showing exactly what each piece does, and how it works to make the code either more concise, or more flexible.\n","\n","MNIST data setup\n","We will use the MNIST dataset and pathlib for dealing with paths (part of the Python 3 standard library), and will download the dataset using requests. We will only import modules when we use them, so you can see exactly what's being used at each point."],"metadata":{"id":"6DExYoNDRqsF"}},{"cell_type":"code","source":["from torchsummary import summary\n","from pathlib import Path\n","import requests\n","\n","DATA_PATH = Path(\"data\")\n","PATH = DATA_PATH / \"mnist\"\n","\n","PATH.mkdir(parents=True, exist_ok=True)\n","\n","URL = \"https://figshare.com/ndownloader/files/25635053\"\n","FILENAME = \"mnist.pkl.gz\"\n","\n","if not (PATH / FILENAME).exists():\n","        content = requests.get(URL).content\n","        (PATH / FILENAME).open(\"wb\").write(content)\n","#Check that the file has been downloaded:\n","\n","!ls data/mnist"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cjTXKuDRuaO","executionInfo":{"status":"ok","timestamp":1695808451967,"user_tz":-120,"elapsed":7,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"577bcd96-e407-41ae-da31-aaf61a887c14"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["mnist.pkl.gz\n"]}]},{"cell_type":"markdown","source":["This dataset is in numpy array format, and has been stored using pickle, a python-specific format for serializing data."],"metadata":{"id":"jk0ZbgSfR1lA"}},{"cell_type":"code","source":["import pickle\n","import gzip\n","\n","with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n","  ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"],"metadata":{"id":"MM21NIdLR9m2","executionInfo":{"status":"ok","timestamp":1695808453145,"user_tz":-120,"elapsed":1181,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["Each image is 28 x 28, and is being stored as a flattened row of length 784 (=28x28). Let's take a look at one; we need to reshape it to 2d first."],"metadata":{"id":"wZnKcnpsSCPu"}},{"cell_type":"code","source":["%matplotlib inline\n","from matplotlib import pyplot as plt\n","import numpy as np\n","\n","print(('x_train.shape',x_train.shape))\n","print(('y_train.shape',y_train.shape))\n","\n","plt.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n","plt.title('class index: ' + str(y_train[0]));"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"id":"DRTzGb-OSFxz","executionInfo":{"status":"ok","timestamp":1695808453480,"user_tz":-120,"elapsed":337,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"f32e18ea-6c70-4c39-a840-4371666cbaf5"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["('x_train.shape', (50000, 784))\n","('y_train.shape', (50000,))\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","\n","x_train, y_train, x_valid, y_valid = map(\n","    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",")\n","n, c = x_train.shape\n","x_train, x_train.shape, y_train.min(), y_train.max()\n","print(('x_train.shape',x_train.shape))\n","print(('y_train.shape',y_train.shape))\n","print(('x_train.type()',x_train.type()))\n","print(('y_train.type()',y_train.type()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8rcEuR80SJ0K","executionInfo":{"status":"ok","timestamp":1695808453481,"user_tz":-120,"elapsed":19,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"26217de1-08a6-40bf-cb39-f894204b174d"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["('x_train.shape', torch.Size([50000, 784]))\n","('y_train.shape', torch.Size([50000]))\n","('x_train.type()', 'torch.FloatTensor')\n","('y_train.type()', 'torch.LongTensor')\n"]}]},{"cell_type":"markdown","source":["Neural net from scratch (no torch.nn)\n","Let's first create a model using nothing but PyTorch tensor operations.\n","\n","Just like numpy, PyTorch provides methods to create random or zero-filled tensors, which we will use to create our weights and bias for a simple linear model. These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n","\n","For the weights, we set requires_grad after the initialization, since we don't want that step included in the gradient. (Note that a trailling _ in PyTorch signifies that the operation is performed in-place.)\n","\n","Note: We are initializing the weights here with Xavier initialisation. This is done by multiplying with 1/sqrt(n)."],"metadata":{"id":"J1C8flBOSgex"}},{"cell_type":"code","source":["# Weight matrix\n","W = torch.randn(784, 10) / np.sqrt(784)\n","W.requires_grad_()\n","\n","# Bias\n","b = torch.zeros(10, requires_grad=True)"],"metadata":{"id":"EzGyZqw4S_nS","executionInfo":{"status":"ok","timestamp":1695808453481,"user_tz":-120,"elapsed":17,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["Thanks to PyTorch's ability to calculate gradients automatically (see Autograd), we can use any standard Python function (or callable object) as a model! So let's just write a plain matrix multiplication and broadcasted addition to create a simple logistic regression model.\n","\n","Note: Although PyTorch provides lots of pre-written loss functions, activation functions, and so forth, you can easily write your own using plain python. PyTorch will even create fast GPU or vectorized CPU code for your function automatically. Here, I have implemented the softmax myself."],"metadata":{"id":"VW1TlO44TAnU"}},{"cell_type":"code","source":["# Note that x is a batch of size 64 x 784\n","def softmax(x):\n","  scores = torch.exp(x)\n","  normalizer = torch.sum(scores, -1, keepdim=True)\n","  probabilities = scores / normalizer\n","  return probabilities\n","\n","# This is our model (called h in the lecture slides)\n","def model(x):\n","    return softmax(x @ W + b)"],"metadata":{"id":"NcvM0bkETc1x","executionInfo":{"status":"ok","timestamp":1695808453481,"user_tz":-120,"elapsed":16,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["In the above, the @ stands for the dot product operation. We will call our function on one batch of data (in this case, 64 images). This is one forward pass. Note that our predictions won't be any better than random at this stage, since we start with random weights."],"metadata":{"id":"AY4IFw04ThuT"}},{"cell_type":"code","source":["bs = 64  # batch size\n","\n","xb = x_train[0:bs] # a mini-batch from x_train\n","preds = model(xb)  # predictions\n","\n","print(preds[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GBQLNJXfTjwZ","executionInfo":{"status":"ok","timestamp":1695808453482,"user_tz":-120,"elapsed":17,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"fa3484f8-b18b-4c9f-a541-922048487920"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.1262, 0.0844, 0.0843, 0.1130, 0.1161, 0.1068, 0.1172, 0.0818, 0.1117,\n","        0.0584], grad_fn=<SelectBackward0>)\n"]}]},{"cell_type":"markdown","source":["Questions 1:\n","Output preds is a tensor. What is its dimensionality, and how should we interpret the values? A: As the predicted class probabilities (softmax)\n","Can you guess what grad_fn=<SelectBackward> means? A: Maybe not the most clear question. This is the function being used when we want to calculate the gradient w.r.t. the model parameters (i.e., when we call loss.backward() below). There is a short explanation of the grad_fn attribute in Lab 5 part 2.\n","Let's implement negative log-likelihood to use as the loss function (again, we can just use standard Python):"],"metadata":{"id":"qKbwMjnhTmfH"}},{"cell_type":"code","source":["def nll(input, target):\n","  pred_select_true = input[range(target.shape[0]),target]\n","  return -torch.log(pred_select_true).mean()\n","\n","loss_func = nll"],"metadata":{"id":"wWtDPSK4Tz4u","executionInfo":{"status":"ok","timestamp":1695808453482,"user_tz":-120,"elapsed":15,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["Recall that in the logistic regression loss, the inner-most sum has two terms that look somewhat like this\n","\n","y*log(p) + (1-y)*log(1-p)\n","Only one of these is none-zero, because the true label (y) is either 0 or 1. This generalizes to softmax, where the number of classes is K. The expression\n","\n","pred_select_true = input[range(target.shape[0]),target]\n","has the effect of picking only the term corresponding to the true label. Hence, the terms corresponding to the incorrect labels do not contribute to the loss in our implementation.\n","\n","Let's check our loss with our random model, so we can see if we improve after a backprop pass later."],"metadata":{"id":"CAaOQIBoT40I"}},{"cell_type":"code","source":["yb = y_train[0:bs]\n","print(loss_func(preds, yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSMTKTdcUF7f","executionInfo":{"status":"ok","timestamp":1695808453482,"user_tz":-120,"elapsed":14,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"c13e8218-d3c2-4d5f-e036-4c4d60660719"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.3441, grad_fn=<NegBackward0>)\n"]}]},{"cell_type":"markdown","source":["Let's also implement a function to calculate the accuracy of our model. For each prediction, if the index with the largest value matches the target value, then the prediction was correct."],"metadata":{"id":"G8ieAeAhUJ9B"}},{"cell_type":"code","source":["def accuracy(out, yb):\n","    preds = torch.argmax(out, dim=1)\n","    return (preds == yb).float().mean()\n","\n","\n","\n","\n","\n","\n","print(accuracy(preds, yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVQz3dq6UMXW","executionInfo":{"status":"ok","timestamp":1695808453482,"user_tz":-120,"elapsed":12,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"30cf7f6d-4871-4b37-f045-3a99df1064b6"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.1250)\n"]}]},{"cell_type":"markdown","source":["We can now run a training loop. For each iteration, we will:\n","\n","select a mini-batch of data (of size bs)\n","use the model to make predictions\n","calculate the loss\n","loss.backward() updates the gradients of the model, in this case, weights and bias.\n","We now use these gradients to update the weights and bias. We do this within the torch.no_grad() context manager, because we do not want these actions to be recorded for our next calculation of the gradient. You can read more about how PyTorch's Autograd records operations here.\n","\n","We then set the gradients to zero, so that we are ready for the next loop. Otherwise, our gradients would record a running tally of all the operations that had happened (i.e. loss.backward() adds the gradients to whatever is already stored, rather than replacing them)."],"metadata":{"id":"fi8qCLhwUYZJ"}},{"cell_type":"code","source":["lr = 0.01  # learning rate\n","epochs = 2  # how many epochs to train for\n","\n","loss_plot = []\n","\n","for epoch in range(epochs):\n","  for i in range((n - 1) // bs + 1):\n","    start_i = i * bs\n","    end_i = start_i + bs\n","    xb = x_train[start_i:end_i]\n","    yb = y_train[start_i:end_i]\n","    pred = model(xb)\n","    loss = loss_func(pred, yb)\n","\n","    loss_plot.append(loss.detach().numpy())\n","\n","    loss.backward()\n","    with torch.no_grad():\n","      W -= W.grad * lr\n","      b -= b.grad * lr\n","      W.grad.zero_()\n","      b.grad.zero_()\n","print(bs)\n","plt.plot(loss_plot)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"id":"8-6y5oDyUfwZ","executionInfo":{"status":"ok","timestamp":1695808454567,"user_tz":-120,"elapsed":1094,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"6fec9bb0-289f-4866-dcce-ef0277b056d6"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["64\n"]},{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x78a713d1e7a0>]"]},"metadata":{},"execution_count":47},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["That's it: we've created and trained a minimal neural network (in this case, a logistic regression, since we have no hidden layers) entirely from scratch!\n","\n","Let's check the loss and accuracy and compare those to what we got earlier. We expect that the loss will have decreased and accuracy to have increased, and they have."],"metadata":{"id":"b2D4NpzaUlAf"}},{"cell_type":"code","source":["print(loss_func(model(xb), yb), accuracy(model(xb), yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eTCZyxjZUzLA","executionInfo":{"status":"ok","timestamp":1695808454567,"user_tz":-120,"elapsed":24,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"d37c31b8-1d64-4d12-adf7-a9c077e2cdae"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.9407, grad_fn=<NegBackward0>) tensor(0.7500)\n"]}]},{"cell_type":"markdown","source":["Using torch.nn.functional\n","We will now refactor our code, so that it does the same thing as before, only we'll start taking advantage of PyTorch's nn classes to make it more concise and flexible. At each step from here, we should be making our code one or more of: shorter, more understandable, and/or more flexible.\n","\n","The first and easiest step is to make our code shorter by replacing our hand-written activation and loss functions with those from torch.nn.functional (which is generally imported into the namespace F by convention). This module contains all the functions in the torch.nn library (whereas other parts of the library contain classes). As well as a wide range of loss and activation functions, you'll also find here some convenient functions for creating neural nets, such as pooling functions. (There are also functions for doing convolutions, linear layers, etc, but as we'll see, these are usually better handled using other parts of the library.)\n","\n","If you're using negative log likelihood loss and softmax activation, then Pytorch provides a single function F.cross_entropy that combines the two. So we can even remove the activation function from our model."],"metadata":{"id":"qGEzt-nlU4Yz"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","loss_func = F.cross_entropy\n","\n","def model(x):\n","    return x @ W + b"],"metadata":{"id":"HcpWJHtxVwup","executionInfo":{"status":"ok","timestamp":1695808454567,"user_tz":-120,"elapsed":17,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":["Note that we no longer call softmax in the model function. Let's confirm that our loss and accuracy are the same as before:"],"metadata":{"id":"WeyGF0ctVyeY"}},{"cell_type":"code","source":["print(loss_func(model(xb), yb), accuracy(model(xb), yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTHEl6pVV2vP","executionInfo":{"status":"ok","timestamp":1695808454568,"user_tz":-120,"elapsed":17,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"28060ca3-1765-49c6-e3ef-43523515e741"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.9407, grad_fn=<NllLossBackward0>) tensor(0.7500)\n"]}]},{"cell_type":"markdown","source":["Refactor using nn.Module\n","Next up, we'll use nn.Module and nn.Parameter, for a clearer and more concise training loop. We subclass nn.Module (which itself is a class and able to keep track of state). In this case, we want to create a class that holds our weights, bias, and method for the forward step. nn.Module has a number of attributes and methods (such as .parameters() and .zero_grad()) which we will be using.\n","\n","nn.Module (uppercase M) is a PyTorch specific concept, and is a class we'll be using a lot. nn.Module is not to be confused with the Python concept of a (lowercase m) module, which is a file of Python code that can be imported."],"metadata":{"id":"wgqCelWeV8Gh"}},{"cell_type":"code","source":["from torch import nn\n","\n","class Mnist_Logistic(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.W = nn.Parameter(torch.randn(784, 10) / np.sqrt(784))\n","        self.b = nn.Parameter(torch.zeros(10))\n","\n","    def forward(self, x):\n","        return x @ self.W + self.b"],"metadata":{"id":"mccTyFOFWCsu","executionInfo":{"status":"ok","timestamp":1695808455176,"user_tz":-120,"elapsed":621,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["Since we're now using an object instead of just using a function, we first have to instantiate our model:"],"metadata":{"id":"zWieSFA5WK9r"}},{"cell_type":"code","source":["model = Mnist_Logistic()"],"metadata":{"id":"5cQ6-TycWCz9","executionInfo":{"status":"ok","timestamp":1695808455176,"user_tz":-120,"elapsed":6,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["Now we can calculate the loss in the same way as before. Note that nn.Module objects are used as if they are functions (i.e they are callable), but behind the scenes PyTorch will call our forward method automatically."],"metadata":{"id":"l2IF0FXNWOzR"}},{"cell_type":"code","source":["print(loss_func(model(xb), yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WaibjdbPWSLf","executionInfo":{"status":"ok","timestamp":1695808455176,"user_tz":-120,"elapsed":6,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"1892e53f-8f07-49a2-cb4f-cdd56d4bc659"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.4105, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["Previously for our training loop we had to update the values for each parameter by name, and manually zero out the grads for each parameter separately, like this:\n","\n","with torch.no_grad():\n","  W -= W.grad * lr\n","  b -= b.grad * lr\n","  W.grad.zero_()\n","  b.grad.zero_()\n","Now we can take advantage of model.parameters() and model.zero_grad(), which are both defined by PyTorch for nn.Module, to make those steps more concise and less prone to the error of forgetting some of our parameters, particularly if we had a more complicated model:\n","\n","with torch.no_grad():\n","  for p in model.parameters(): p -= p.grad * lr\n","  model.zero_grad()\n","We'll wrap our little training loop in a fit function so we can run it again later."],"metadata":{"id":"4urFDQXcWVvU"}},{"cell_type":"code","source":["def fit():\n","  for epoch in range(epochs):\n","    for i in range((n - 1) // bs + 1):\n","      start_i = i * bs\n","      end_i = start_i + bs\n","      xb = x_train[start_i:end_i]\n","      yb = y_train[start_i:end_i]\n","      pred = model(xb)\n","      loss = loss_func(pred, yb)\n","\n","      loss.backward()\n","      with torch.no_grad():\n","        for p in model.parameters():\n","          p -= p.grad * lr\n","        model.zero_grad()\n","\n","fit()\n","\n","print(loss_func(model(xb), yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s5J4rfAiWWUC","executionInfo":{"status":"ok","timestamp":1695808455426,"user_tz":-120,"elapsed":255,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"d1cff141-934e-4b1c-c3e5-f6a0b2b5eb6c"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.9621, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["Refactor using nn.Linear\n","We continue to refactor our code. Instead of manually defining and initializing self.W and self.b, and calculating x @ self.W + self.b, we will instead use the PyTorch class nn.Linear for a linear layer, which does all that for us. Like Keras, PyTorch has many types of predefined layers that can greatly simplify our code, and often makes it faster too."],"metadata":{"id":"TpLjR5MyWjzL"}},{"cell_type":"code","source":["class Mnist_Logistic(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.lin = nn.Linear(784, 10)\n","\n","    def forward(self, x):\n","        return self.lin(x)"],"metadata":{"id":"2ly9K_52XFn7","executionInfo":{"status":"ok","timestamp":1695808455426,"user_tz":-120,"elapsed":4,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":["Questions 2:\n","What is the equivalent of Linear in the Keras framework?  dense"],"metadata":{"id":"jEpeRuljXPfM"}},{"cell_type":"code","source":["model = Mnist_Logistic()\n","print(loss_func(model(xb), yb))\n","fit()\n","\n","print(loss_func(model(xb), yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66wJpJnMXR9g","executionInfo":{"status":"ok","timestamp":1695808455689,"user_tz":-120,"elapsed":267,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"fa16f3c6-c0a8-41a5-b320-b7a49523440b"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.2803, grad_fn=<NllLossBackward0>)\n","tensor(0.9395, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["Refactor using optim\n","Pytorch also has a package with various optimization algorithms, torch.optim. We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter.\n","\n","This will let us replace our previous manually coded optimization step:\n","\n","with torch.no_grad():\n","  for p in model.parameters(): p -= p.grad * lr\n","  model.zero_grad()\n","and instead use just:\n","\n","opt.step()\n","opt.zero_grad()\n","optim.zero_grad() resets the gradient to 0 and we need to call it before computing the gradient for the next minibatch."],"metadata":{"id":"TOI7nGoEXWr1"}},{"cell_type":"code","source":["from torch import optim\n","#We'll define a little function to create our model and optimizer so we can reuse it in the future.\n","\n","def get_model():\n","    model = Mnist_Logistic()\n","    return model, optim.SGD(model.parameters(), lr=lr)\n","\n","model, opt = get_model()\n","print(loss_func(model(xb), yb))\n","\n","for epoch in range(epochs):\n","    for i in range((n - 1) // bs + 1):\n","        start_i = i * bs\n","        end_i = start_i + bs\n","        xb = x_train[start_i:end_i]\n","        yb = y_train[start_i:end_i]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","print(loss_func(model(xb), yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufwdFZOyXrlK","executionInfo":{"status":"ok","timestamp":1695808456358,"user_tz":-120,"elapsed":671,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"4f068ddd-1ca2-47da-f927-1016c017d7cc"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.2960, grad_fn=<NllLossBackward0>)\n","tensor(0.9346, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["Refactor using Dataset\n","PyTorch has an abstract Dataset class. A Dataset can be anything that has a __len__ function (called by Python's standard len function) and a __getitem__ function as a way of indexing into it.\n","\n","This tutorial walks through a nice example of creating a custom FacialLandmarkDataset class as a subclass of Dataset.\n","\n","PyTorch's TensorDataset is a Dataset wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train."],"metadata":{"id":"jiyA0oJxXz-G"}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset\n","#Both x_train and y_train can be combined in a single TensorDataset, which will be easier to iterate over and slice.\n","\n","train_ds = TensorDataset(x_train, y_train)"],"metadata":{"id":"GjPSyZbdX2b7","executionInfo":{"status":"ok","timestamp":1695808456359,"user_tz":-120,"elapsed":12,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["Previously, we had to iterate through minibatches of x and y values separately:\n","\n","xb = x_train[start_i:end_i]\n","yb = y_train[start_i:end_i]\n","Now, we can do these two steps together:\n","\n","xb,yb = train_ds[i*bs : i*bs+bs]"],"metadata":{"id":"ArWlM5enX7Wm"}},{"cell_type":"code","source":["model, opt = get_model()\n","\n","for epoch in range(epochs):\n","    for i in range((n - 1) // bs + 1):\n","        xb, yb = train_ds[i * bs: i * bs + bs]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","print(loss_func(model(xb), yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPL4-ruaYRl6","executionInfo":{"status":"ok","timestamp":1695808456890,"user_tz":-120,"elapsed":541,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"46520261-1d51-4a5b-9788-22345ae96c6a"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.9525, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["Refactor using DataLoader\n","PyTorch's DataLoader is responsible for managing batches. You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches. Rather than having to use train_ds[i*bs : i*bs+bs], the DataLoader gives us each minibatch automatically."],"metadata":{"id":"jHMaHa7BYU3Q"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=bs)\n","\n","model, opt = get_model()\n","\n","for epoch in range(epochs):\n","    for xb, yb in train_dl:\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","print(loss_func(model(xb), yb))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V5n68i9_YbNE","executionInfo":{"status":"ok","timestamp":1695808458614,"user_tz":-120,"elapsed":1730,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"f4a8efa5-5b75-4695-8f05-7df6d97aa15d"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.9334, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["Thanks to Pytorch's nn.Module, nn.Parameter, Dataset, and DataLoader, our training loop is now dramatically smaller and easier to understand. Let's now try to add the basic features necessary to create effecive models in practice.\n","\n","Add validation\n","Above we were just trying to get a reasonable training loop set up for use on our training data. In reality, you always should also have a validation set in order to identify if you are overfitting.\n","\n","Shuffling the training data is important to prevent correlation between batches and overfitting. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n","\n","We'll use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn't need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly."],"metadata":{"id":"xOEdDd1YYhOS"}},{"cell_type":"code","source":["\n","train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n","\n","valid_ds = TensorDataset(x_valid, y_valid)\n","valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"],"metadata":{"id":"jiZ8w0j9Ypm7","executionInfo":{"status":"ok","timestamp":1695808458614,"user_tz":-120,"elapsed":4,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":["We will calculate and print the validation loss at the end of each epoch.\n","\n","(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)"],"metadata":{"id":"RxO0uJD8Yqgb"}},{"cell_type":"code","source":["model, opt = get_model()\n","\n","for epoch in range(epochs):\n","    model.train()\n","    for xb, yb in train_dl:\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","    model.eval()\n","    with torch.no_grad():\n","        valid_acc = sum(accuracy(model(xb), yb) for xb, yb in valid_dl)\n","        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n","        print('epoch',epoch, 'val loss', float(valid_loss / len(valid_dl)), 'val acc.', float(valid_acc / len(valid_dl)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1YDgsAGYuAD","executionInfo":{"status":"ok","timestamp":1695808460307,"user_tz":-120,"elapsed":1696,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"dfb539ff-288c-4bd5-ccef-dd711816723a"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0 val loss 0.6317651271820068 val acc. 0.8726266026496887\n","epoch 1 val loss 0.4878411591053009 val acc. 0.8867682218551636\n"]}]},{"cell_type":"markdown","source":["Create fit() and get_data()\n","We'll now do a little refactoring of our own. Since we go through a similar process twice of calculating the loss for both the training set and the validation set, let's make that into its own function, loss_batch, which computes the loss for one batch.\n","\n","We pass an optimizer in for the training set, and use it to perform backprop. For the validation set, we don't pass an optimizer, so the method doesn't perform backprop."],"metadata":{"id":"qzvMBcbmY05a"}},{"cell_type":"code","source":["def loss_batch(model, loss_func, xb, yb, opt=None):\n","    loss = loss_func(model(xb), yb)\n","\n","    if opt is not None:\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","    return loss.item(), len(xb)\n","#fit runs the necessary operations to train our model and computes the training and validation losses for each epoch.\n","\n","def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n","    for epoch in range(epochs):\n","        model.train()\n","        for xb, yb in train_dl:\n","            loss_batch(model, loss_func, xb, yb, opt)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            losses, nums = zip(\n","                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n","            )\n","        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n","\n","        print(epoch, val_loss)\n","#get_data returns dataloaders for the training and validation sets.\n","\n","def get_data(train_ds, valid_ds, bs):\n","    return (\n","        DataLoader(train_ds, batch_size=bs, shuffle=True),\n","        DataLoader(valid_ds, batch_size=bs * 2),\n","    )"],"metadata":{"id":"JO7pZDTJY3FK","executionInfo":{"status":"ok","timestamp":1695808460710,"user_tz":-120,"elapsed":406,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":["Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"],"metadata":{"id":"u48WrWGUZDvO"}},{"cell_type":"code","source":["train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","model, opt = get_model()\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"egVmZV7jZHBt","executionInfo":{"status":"ok","timestamp":1695808462596,"user_tz":-120,"elapsed":1889,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"2b8e3e7b-fa5b-450f-bf34-73b7fe5c0e37"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.6313147482872009\n","1 0.48936862115859986\n"]}]},{"cell_type":"markdown","source":["You can use these basic 3 lines of code to train a wide variety of models. Let's see if we can use them to train a convolutional neural network (CNN)!\n","\n","Switch to CNN\n","We are now going to build our neural network with three convolutional layers. Because none of the functions in the previous section assume anything about the model form, we'll be able to use them to train a CNN without any modification.\n","\n","We will use Pytorch's predefined Conv2d class as our convolutional layer. We define a CNN with 3 convolutional layers. Each convolution is followed by a ReLU. At the end, we perform an average pooling. (Note that view is PyTorch's version of numpy's reshape)"],"metadata":{"id":"2SKzAhpGZME5"}},{"cell_type":"code","source":["class Mnist_CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n","        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n","\n","    def forward(self, xb):\n","        xb = xb.view(-1, 1, 28, 28)\n","        xb = F.relu(self.conv1(xb))\n","        xb = F.relu(self.conv2(xb))\n","        xb = F.relu(self.conv3(xb))\n","        xb = F.avg_pool2d(xb, 4)\n","        return xb.view(-1, xb.size(1)) # view = reshape\n","\n","lr = 0.1\n","\n","\n"],"metadata":{"id":"PINSF4XTZXkD","executionInfo":{"status":"ok","timestamp":1695808462596,"user_tz":-120,"elapsed":7,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["model = Mnist_CNN()\n","summary(model.cuda(), (1, 28, 28), batch_size=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iXlNP66VDigD","executionInfo":{"status":"ok","timestamp":1695808462596,"user_tz":-120,"elapsed":6,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"5d2559f0-e8a3-432e-9570-4684cb5a7555"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 14, 14]             160\n","            Conv2d-2             [-1, 16, 7, 7]           2,320\n","            Conv2d-3             [-1, 10, 4, 4]           1,450\n","================================================================\n","Total params: 3,930\n","Trainable params: 3,930\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.03\n","Params size (MB): 0.01\n","Estimated Total Size (MB): 0.05\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["Momentum is a variation on stochastic gradient descent that takes previous updates into account as well and generally leads to faster training."],"metadata":{"id":"adLsn1obZb2Z"}},{"cell_type":"code","source":["model = Mnist_CNN()\n","#summary(model.cuda(), (1, 28, 28), batch_size=-1)\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s8JT6zBTahH6","executionInfo":{"status":"ok","timestamp":1695808471339,"user_tz":-120,"elapsed":8747,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"05d3df83-a2f4-4fbe-f119-b2df6c57b0d0"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.35919719994068144\n","1 0.24232865868806838\n"]}]},{"cell_type":"markdown","source":["nn.Sequential\n","torch.nn has another handy class we can use to simply our code: Sequential. A Sequential object runs each of the modules contained within it, in a sequential manner. This is a simpler way of writing our neural network.\n","\n","To take advantage of this, we need to be able to easily define a custom layer from a given function. For instance, PyTorch doesn't have a view layer, and we need to create one for our network. Lambda will create a layer that we can then use when defining a network with Sequential."],"metadata":{"id":"nBPR9DRFajkA"}},{"cell_type":"code","source":["class Lambda(nn.Module):\n","    def __init__(self, func):\n","        super().__init__()\n","        self.func = func\n","\n","    def forward(self, x):\n","        return self.func(x)\n","\n","def preprocess(x):\n","    return x.view(-1, 1, 28, 28)\n","#The model created with Sequential is simply:\n","\n","model = nn.Sequential(\n","    Lambda(preprocess),\n","    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.AvgPool2d(4),\n","    Lambda(lambda x: x.view(x.size(0), -1)),\n",")\n","\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JNxDQJdXamFN","executionInfo":{"status":"ok","timestamp":1695808480664,"user_tz":-120,"elapsed":9333,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"1c086440-2de5-4929-db23-4d3da33d920f"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.32204685306549075\n","1 0.2740514630556107\n"]}]},{"cell_type":"markdown","source":["Wrapping DataLoader\n","Our CNN is fairly concise, but it only works with MNIST, because:\n","\n","It assumes the input is a 28*28 long vector\n","It assumes that the final CNN grid size is 4*4 (since that's the average\n","pooling kernel size we used)\n","\n","Let's get rid of these two assumptions, so our model works with any 2d single channel image. First, we can remove the initial Lambda layer by moving the data preprocessing into a generator:"],"metadata":{"id":"7Qr8K1rAa0-I"}},{"cell_type":"code","source":["def preprocess(x, y):\n","    return x.view(-1, 1, 28, 28), y\n","\n","class WrappedDataLoader:\n","    def __init__(self, dl, func):\n","        self.dl = dl\n","        self.func = func\n","\n","    def __len__(self):\n","        return len(self.dl)\n","\n","    def __iter__(self):\n","        batches = iter(self.dl)\n","        for b in batches:\n","            yield (self.func(*b))\n","\n","train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","train_dl = WrappedDataLoader(train_dl, preprocess)\n","valid_dl = WrappedDataLoader(valid_dl, preprocess)"],"metadata":{"id":"l2bVDpYNqLP8","executionInfo":{"status":"ok","timestamp":1695808480665,"user_tz":-120,"elapsed":4,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":["Next, we can replace nn.AvgPool2d with nn.AdaptiveAvgPool2d, which allows us to define the size of the output tensor we want, rather than the input tensor we have. As a result, our model will work with any size input."],"metadata":{"id":"YYp2sXsnqOXx"}},{"cell_type":"code","source":["model = nn.Sequential(\n","    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.AdaptiveAvgPool2d(1),\n","    Lambda(lambda x: x.view(x.size(0), -1)),\n",")\n","\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uz_xzbXAqQdj","executionInfo":{"status":"ok","timestamp":1695808489895,"user_tz":-120,"elapsed":9233,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"b929f373-e36c-4f4a-e0d3-e564e94e932d"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.42317541279792786\n","1 0.3033162287533283\n"]}]},{"cell_type":"markdown","source":["Using your GPU\n","If you're lucky enough to have access to a CUDA-capable GPU (you can rent one for about $0.50/hour from most cloud providers) you can use it to speed up your code. First check that your GPU is working in Pytorch:"],"metadata":{"id":"jdMZ0QCXqaaa"}},{"cell_type":"code","source":["print(torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwA4uW6Tqdvt","executionInfo":{"status":"ok","timestamp":1695808489896,"user_tz":-120,"elapsed":21,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"ff1c6281-ed23-4fd4-fb54-f3817dce0fa5"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["\n","dev = torch.device(\n","    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"],"metadata":{"id":"LScH5ZmvqfU9","executionInfo":{"status":"ok","timestamp":1695808489896,"user_tz":-120,"elapsed":16,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["def preprocess(x, y):\n","    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n","\n","train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","train_dl = WrappedDataLoader(train_dl, preprocess)\n","valid_dl = WrappedDataLoader(valid_dl, preprocess)\n"],"metadata":{"id":"R0aFZz9Nqg9T","executionInfo":{"status":"ok","timestamp":1695808489896,"user_tz":-120,"elapsed":15,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["\n","summary(model.cuda(), (1, 28, 28), batch_size=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWNFyYcRDyvK","executionInfo":{"status":"ok","timestamp":1695808489896,"user_tz":-120,"elapsed":13,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"b3d8e1a9-07f3-4659-cb07-02378c694037"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 14, 14]             160\n","              ReLU-2           [-1, 16, 14, 14]               0\n","            Conv2d-3             [-1, 16, 7, 7]           2,320\n","              ReLU-4             [-1, 16, 7, 7]               0\n","            Conv2d-5             [-1, 10, 4, 4]           1,450\n","              ReLU-6             [-1, 10, 4, 4]               0\n"," AdaptiveAvgPool2d-7             [-1, 10, 1, 1]               0\n","            Lambda-8                   [-1, 10]               0\n","================================================================\n","Total params: 3,930\n","Trainable params: 3,930\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.06\n","Params size (MB): 0.01\n","Estimated Total Size (MB): 0.08\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["model.to(dev)\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","#You should find it runs faster now:\n","\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UfK4Ag8kq1s4","executionInfo":{"status":"ok","timestamp":1695808494198,"user_tz":-120,"elapsed":4310,"user":{"displayName":"Anne Andresen","userId":"01547380821933232406"}},"outputId":"3f35399b-9c24-4998-dbb8-1fcd33a2e351"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.23482999778985977\n","1 0.19776384092569352\n"]}]},{"cell_type":"markdown","source":["Closing thoughts\n","We now have a general data pipeline and training loop which you can use for training many types of models using Pytorch. To see how simple training a model can now be, take a look at the mnist_sample sample notebook.\n","\n","Of course, there are many things you'll want to add, such as data augmentation, hyperparameter tuning, monitoring training, transfer learning, and so forth. These features are available in the fastai library, which has been developed using the same design approach shown in this tutorial, providing a natural next step for practitioners looking to take their models further.\n","\n","We promised at the start of this tutorial we'd explain through example each of torch.nn, torch.optim, Dataset, and DataLoader. So let's summarize what we've seen:\n","\n","torch.nn\n","\n","Module: creates a callable which behaves like a function, but can also contain state(such as neural net layer weights). It knows what Parameter (s) it contains and can zero all their gradients, loop through them for weight updates, etc.\n","Parameter: a wrapper for a tensor that tells a Module that it has weights that need updating during backprop. Only tensors with the requires_grad attribute set are updated\n","functional: a module(usually imported into the F namespace by convention) which contains activation functions, loss functions, etc, as well as non-stateful versions of layers such as convolutional and linear layers.\n","torch.optim: Contains optimizers such as SGD, which update the weights of Parameter during the backward step\n","\n","Dataset: An abstract interface of objects with a __len__ and a __getitem__, including classes provided with Pytorch such as TensorDataset\n","\n","DataLoader: Takes any Dataset and creates an iterator which returns batches of data."],"metadata":{"id":"SIXtbexSq5Zn"}}]}